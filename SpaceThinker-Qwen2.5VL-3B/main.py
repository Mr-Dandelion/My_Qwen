import torch
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
import requests
from io import BytesIO
import argparse

# Argument parsing
parser = argparse.ArgumentParser()
parser.add_argument("--model_path", type=str, required=True)
parser.add_argument("--image_path", type=str, required=True)
args = parser.parse_args()
model_path = args.model_path
image_path = args.image_path

# Prompt system setup
system_message = (
    "You are VL-Thinking ğŸ¤”, a helpful assistant with excellent reasoning ability. "
    "You should first think about the reasoning process and then provide the answer. "
    "Use <think>...</think> and <answer>...</answer> tags."
)

# Load model and processor
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_path, device_map="auto", torch_dtype=torch.float16
)
processor = AutoProcessor.from_pretrained(model_path)

# Load and preprocess image
if image_path.startswith("http"):
    image = Image.open(BytesIO(requests.get(image_path).content)).convert("RGB")
else:
    image = Image.open(image_path).convert("RGB")
if image.width > 512:
    ratio = image.height / image.width
    image = image.resize((512, int(512 * ratio)), Image.Resampling.LANCZOS)

# Initialize multi-turn chat history
chat = [
    {"role": "system", "content": [{"type": "text", "text": system_message}]},
    {"role": "user", "content": [{"type": "image", "image": image}]}
]

print("ğŸ’¬ å¤šè½®å¯¹è¯æ¨¡å¼å·²å¯åŠ¨ï¼Œè¾“å…¥ä½ çš„é—®é¢˜ï¼ˆè¾“å…¥ 'exit' æˆ– 'quit' ç»“æŸï¼‰ï¼š\n")

while True:
    user_input = input("You: ").strip()
    if user_input.lower() in {"exit", "quit"}:
        print("ğŸ‘‹ é€€å‡ºå¯¹è¯")
        break

    # è¿½åŠ ç”¨æˆ·è¾“å…¥åˆ°å¯¹è¯å†å²
    chat.append({"role": "user", "content": [{"type": "text", "text": user_input}]})

    # æ„é€ è¾“å…¥
    text_input = processor.apply_chat_template(
        chat, tokenize=False, add_generation_prompt=True
    )
    inputs = processor(text=[text_input], images=[image], return_tensors="pt").to("cuda")

    # ç”Ÿæˆå›å¤
    generated_ids = model.generate(**inputs, max_new_tokens=1024)
    output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print("Assistant:\n", output + "\n")

    # è¿½åŠ æ¨¡å‹å›å¤åˆ°å¯¹è¯å†å²
    chat.append({"role": "assistant", "content": [{"type": "text", "text": output}]})

